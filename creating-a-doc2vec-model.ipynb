{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating a Doc2Vec Model**\n",
    "\n",
    "The joy of word2vec is that it will retain the context within the paragraphs, resulting in more meaningful vector values. My plan was to use this to quickly group documents together, making it faster to find resources. \n",
    "\n",
    "The first couple of word2vec / doc2vec models I attempted to build were a bit of a struggle, so I thought it might be useful for people to see the process here. \n",
    "\n",
    "At the end of this notebook the model is saved as a .model file, so there's no need to run the long training regime again, you can just laod the model and off you go. \n",
    "\n",
    "Other ideas were to create a new feature that represents the average vector for each text body, using this to cluster the documents together and possibly assign labels to begin automating coronavirus risk extraction. \n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "As ever the first thing to do is import the libraries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve the desired result we only need a few modules. \n",
    "\n",
    "1. Gensim: An amazing word2vec / doc2vec library that allows you to build your own d2v models, as well as load pre-trained models. Brilliant documentation as well. \n",
    "\n",
    "2. NLTK: Brilliant NLP library! Has everythig the aspiring NLP magician needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace NaN Function**\n",
    "\n",
    "I use this function later in the notebook to replace an empty string with a np.nan object. This allows us to easily remove missing values using built-in pandas methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_none(X):\n",
    "    if X == '':\n",
    "        X = np.nan\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model Function**\n",
    "\n",
    "To keep things tidy I put the training of the model into a little function. \n",
    "\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_epochs, vec_size, alpha, tagged_data):\n",
    "    \n",
    "    model = Doc2Vec(vector_size=vec_size,\n",
    "               alpha=alpha,\n",
    "               min_alpha=0.00025,\n",
    "               min_count=1,\n",
    "               dm=1)\n",
    "    \n",
    "    model.build_vocab(tag_data)\n",
    "    \n",
    "    # With the model built we simply train on the data.\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"Iteration {epoch}\")\n",
    "        model.train(tag_data,\n",
    "                   total_examples=model.corpus_count,\n",
    "                   epochs=model.epochs)\n",
    "\n",
    "        # Here I decrease the learning rate. \n",
    "\n",
    "        model.alpha -= 0.0002\n",
    "\n",
    "        model.min_alpha = model.alpha\n",
    "    \n",
    "    # Now simply save the model to avoid training again. \n",
    "    \n",
    "    model.save(\"COVID_MEDICAL_DOCS_w2v_MODEL.model\")\n",
    "    print(\"Model Saved\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Data**\n",
    "\n",
    "The data is loaded from the .csv file that was created in a previous kernel. \n",
    "\n",
    "https://www.kaggle.com/fmitchell259/create-corona-csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "corona_df = pd.read_csv(\"../input/covid19-medical-paperscsv/kaggle_covid-19_open_csv_format.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Corpus**\n",
    "\n",
    "In order to build the model we need to provide the doc2vec object with the entire corpus. \n",
    "\n",
    "All paper text body and titles are used to build a skip-gram model.\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "First though, I check the null values from the parsing. I've lost 944 titles, but for the purposes of building a corpus, we are just within the 5% range. \n",
    "\n",
    "The large missing values in abstract is negligable, as the text_body will reflect the abstract. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0       0\n",
       "doc_id           1\n",
       "source           1\n",
       "title          944\n",
       "abstract      8783\n",
       "text_body        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I drop these values and gather all the data to train the d2v model using my wee function above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_df['title'] = corona_df['title'].apply(replace_none)\n",
    "corona_df['text_body'] = corona_df['text_body'].apply(replace_none)\n",
    "corona_df = corona_df.dropna()\n",
    "\n",
    "w2v_data_body = list(corona_df['text_body'])\n",
    "w2v_data_title = list(corona_df['title'])\n",
    "\n",
    "w2v_total_data = w2v_data_body + w2v_data_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we need to use the Gensim DocumeNt Tagger to apply some typical preprocessing steps for any NLP system. \n",
    "\n",
    "1. Tokenise: Split paragraphs into tokens (each seperate word is a token), making all the words lower case. \n",
    "\n",
    "2. Stemming: The word_tokenize function takes care of stemming. \n",
    "\n",
    "3. Stopword Removel: Likewise the word_tokenzie function takes care of this step. \n",
    "\n",
    "----------------------------\n",
    "\n",
    "Be warned, if you're doing this yourself, this can take a while depending on computing power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(w2v_total_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting Up The Model**\n",
    "\n",
    "With the data all in the one list we can go ahead and  create a model using custom parameters. For more on these parameters check the gensim documentation. It's really good!\n",
    "\n",
    "https://radimrehurek.com/gensim/auto_examples/index.html\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Again, be warned, this can take a while depending on compute power, for this reason I have set the epochs very low. This number is enough to generate sufficient word vectors, however it can be tuned as required. Also note that there is no need for you to run this again (unless you require more epochs), as it is very simple to load a pre-trained doc2vec model, as demonstrated at the end of this notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model = build_model(max_epochs=5, vec_size=10, alpha=0.025, tagged_data=tag_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Demo**\n",
    "\n",
    "So now the model is built we can ask it to vectorise unseen documents, return documents with similiar vectors or maybe create some features as mentioned above. \n",
    "\n",
    "--------------------------------\n",
    "\n",
    "As a quick demo here are the words with which we can test the model, the words chosen have a particular relation to the task at hand (number 2, the risks).\n",
    "\n",
    "I think you'll agree, even at this first pass, the results are interesting. \n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "1. risk\n",
    "2. symptoms\n",
    "3. pregnant\n",
    "4. economy\n",
    "5. isolation\n",
    "\n",
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('misdiagnosis', 0.9361895322799683),\n",
       " ('morbidity', 0.9353204369544983),\n",
       " ('under-reporting', 0.9350523948669434),\n",
       " ('burden', 0.9327030777931213),\n",
       " ('worsened', 0.9271828532218933),\n",
       " ('time-reversal', 0.9211792945861816),\n",
       " ('contagion', 0.9204429984092712),\n",
       " ('recurrence', 0.9195904731750488),\n",
       " ('noncompliance', 0.9159669876098633),\n",
       " ('intractable', 0.9145318269729614)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patients', 0.97841477394104),\n",
       " ('alri', 0.9743953943252563),\n",
       " ('wheeze', 0.9724975824356079),\n",
       " ('vap', 0.965282678604126),\n",
       " ('neutropenia', 0.9645993709564209),\n",
       " ('women', 0.961195170879364),\n",
       " ('suspicion', 0.9602689146995544),\n",
       " ('illness', 0.9602102041244507),\n",
       " ('signs', 0.9587470889091492),\n",
       " ('asthmatics', 0.9583361744880676)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"symptoms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('low-risk', 0.9651879668235779),\n",
       " ('adult', 0.9551292657852173),\n",
       " ('adolescent', 0.951563835144043),\n",
       " ('extra-pulmonary', 0.9502637386322021),\n",
       " ('0/18', 0.9484106302261353),\n",
       " ('87,5', 0.9415570497512817),\n",
       " ('community-dwelling', 0.9341083765029907),\n",
       " ('post-weaning', 0.9336773157119751),\n",
       " ('non-formal', 0.9312962889671326),\n",
       " ('new-onset', 0.9306818246841431)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"pregnant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n=603', 0.9555159211158752),\n",
       " ('venues', 0.9535311460494995),\n",
       " ('war', 0.952997088432312),\n",
       " ('cys-172', 0.9462182521820068),\n",
       " ('industrialization', 0.9448957443237305),\n",
       " ('crises', 0.944724440574646),\n",
       " ('catastrofe', 0.9416725039482117),\n",
       " ('disasters', 0.9395155906677246),\n",
       " ('threat', 0.9342054128646851),\n",
       " ('thailand-myanmar', 0.9253316521644592)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"economy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quadrivalent', 0.9394142031669617),\n",
       " ('trivalent', 0.9388514757156372),\n",
       " ('injecting', 0.9384445548057556),\n",
       " ('cultivation', 0.9381576776504517),\n",
       " ('scrub', 0.9368630051612854),\n",
       " ('rescreened', 0.9354759454727173),\n",
       " ('withheld', 0.9337412118911743),\n",
       " ('intervation', 0.9335280656814575),\n",
       " ('inter-society', 0.9333353042602539),\n",
       " ('practiced', 0.9179999828338623)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"isolation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And That's It!**\n",
    "\n",
    "It's as simple as that to construct meaningful word vectors for documents. I really hope this helps someone get up and running with this data faster. I love building these wee tools so will aim to post anythng that might be remotely useful.\n",
    "\n",
    "-------------------------------------------\n",
    "\n",
    "Bear in mind the functionality you get with NLTK and Gensim is enormous, they are absoutely brilliat and powerful NLP libraries that are incredibly useful, well documented and - to be honest - an outright joy to use. \n",
    "\n",
    "---------------------------------------------------------------\n",
    "\n",
    "Please note that this model only ran for 5 iterations, and while this is good enough to achieve the needed word vectors (and easier on the kernel CPU), this parameter can be tuned as required. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
