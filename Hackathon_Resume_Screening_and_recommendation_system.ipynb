{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "760b5e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sefineh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "915bb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df=pd.read_csv('dataset/resume.csv')\n",
    "resume_copied=resume_df.copy()\n",
    "resume_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "395a83a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = set(stopwords.words('english')+['``',\"''\"])\n",
    "def clean_resume_text(resume):\n",
    "    resume = resume.lower()\n",
    "    resume = re.sub('http\\S+\\s*',' ',resume) #to remove url\n",
    "    resume = ''.join([w for w in resume if not w.isdigit()]) # remove the digits\n",
    "    resume = re.sub('RT|cc',' ',resume) # to remove RT and cc\n",
    "    resume = re.sub('#\\S+','',resume) # to remove hastags\n",
    "    resume = re.sub('@\\S+',' ',resume) # to remove mentions\n",
    "    resume = ''.join([w for w in resume if w not in string.punctuation])# to remove puntuations\n",
    "    resume = re.sub('\\W',' ',resume)\n",
    "    #resume = ''.join([w for w in resume if w not in stopwords_set])\n",
    "    resume = re.sub(r'[^\\x00-\\x7f]',r' ',resume)\n",
    "    resume = re.sub('\\s+',' ',resume)# to remove extra spaces\n",
    "    return resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "198004f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df['Resume']=resume_df.Resume.apply(lambda x: clean_resume_text(x))\n",
    "labelEncoder=LabelEncoder()\n",
    "temp_category=resume_df['Category'].copy()\n",
    "resume_df[\"Category\"]=labelEncoder.fit_transform(resume_df[\"Category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d056f55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: 'Data Science',\n",
       " 12: 'HR',\n",
       " 0: 'Advocate',\n",
       " 1: 'Arts',\n",
       " 24: 'Web Designing',\n",
       " 16: 'Mechanical Engineer',\n",
       " 22: 'Sales',\n",
       " 14: 'Health and fitness',\n",
       " 5: 'Civil Engineer',\n",
       " 15: 'Java Developer',\n",
       " 4: 'Business Analyst',\n",
       " 21: 'SAP Developer',\n",
       " 2: 'Automation Testing',\n",
       " 11: 'Electrical Engineering',\n",
       " 18: 'Operations Manager',\n",
       " 20: 'Python Developer',\n",
       " 8: 'DevOps Engineer',\n",
       " 17: 'Network Security Engineer',\n",
       " 19: 'PMO',\n",
       " 7: 'Database',\n",
       " 13: 'Hadoop',\n",
       " 10: 'ETL Developer',\n",
       " 9: 'DotNet Developer',\n",
       " 3: 'Blockchain',\n",
       " 23: 'Testing'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('dict.pickle', 'rb') as handle:\n",
    "    dict = pickle.load(handle)\n",
    "\n",
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6fa05065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000\n",
    "sequence_length = 300\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "vectorize_layer.adapt(resume_df[\"Resume\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fbcbd34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize_layer.get_vocabulary()\n",
    "train_set,test_set=train_test_split(resume_df,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86dccdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=300\n",
    "model = keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"),\n",
    "  tf.keras.layers.GlobalAveragePooling1D(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(64,activation='relu'),\n",
    "  tf.keras.layers.Dense(32,activation='relu'),\n",
    "  tf.keras.layers.Dense(25,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ec924ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.convert_to_tensor(train_set[\"Resume\"].values)\n",
    "target=tf.convert_to_tensor(train_set[\"Category\"].values)\n",
    "# val_set=tf.convert_to_tensor(val_set[\"Resume\"].values)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6abbc24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "25/25 [==============================] - 1s 21ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 2/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 3/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 4/15\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 5/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 6/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 7/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 8/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 10/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 11/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 12/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "25/25 [==============================] - 0s 19ms/step - loss: 12.8733 - accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "25/25 [==============================] - 0s 18ms/step - loss: 12.8733 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe8ed7f5d90>"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,target,epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "bec8f2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 22ms/step - loss: 13.1581 - accuracy: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13.158133506774902, 0.9948186278343201]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tf.convert_to_tensor(test_set[\"Resume\"]),tf.convert_to_tensor(test_set[\"Category\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a50ff98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p savedmodel\n",
    "model.save(\"savedmodel/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d1bd7309",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_model=tf.keras.models.load_model(\"savedmodel/model\",\\\n",
    "                                     custom_objects={'vectorize_layer':vectorize_layer,\\\n",
    "                                                    \"custom_standardization\":custom_standardization})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "9d6b95a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resume belongs to Web Designing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import textract\n",
    "text = textract.process(\"My resume Intern.pdf\")\n",
    "text=clean_resume_text(str(text))\n",
    "predicted=model.predict(tf.convert_to_tensor([text])).argmax()\n",
    "print(f\"The resume belongs to {dict[predicted]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1b0df03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Data Science\n"
     ]
    }
   ],
   "source": [
    "text=resume_df[\"Resume\"]\n",
    "text=text.values[0]\n",
    "predicted=model.predict(tf.convert_to_tensor([text]))\n",
    "\n",
    "print(predicted.argmax())\n",
    "print(dict[new_model.predict([text]).argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "29228b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First recommended: ==> Data Science\n",
      "The Second recommended: ==> DevOps Engineer\n",
      "The Third recommended: ==> Web Designing\n",
      "The Fourth recommended: ==> Hadoop\n",
      "The Fifth recommended: ==> DotNet Developer\n"
     ]
    }
   ],
   "source": [
    "encodings=np.argpartition(predicted[0],-5)[-5:]\n",
    "encodings=encodings[np.argsort(predicted[0][encodings])]\n",
    "encodings=reversed(encodings)\n",
    "index=0\n",
    "potentials={1:\"First\",2:\"Second\",3:\"Third\",4:\"Fourth\",5:\"Fifth\"}\n",
    "for encoding in encodings:\n",
    "    index+=1\n",
    "    print(f\"The {potentials[index]} recommended: ==>\",dict[encoding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9ea6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd029956",
   "metadata": {},
   "source": [
    "# Find the most relevant resumes that match the job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "99f19ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description=\"\"\"\n",
    "\n",
    "\n",
    "You will have the opportunity to join an artificial intelligence (AI) company for life sciences, that supports the\n",
    "\n",
    "industry in bringing the right drug to the right patient at speed.\n",
    "\n",
    "THE ROLE\n",
    "\n",
    "By joining an established team of data scientists as an NLP specialist, you will:\n",
    "Work closely with both data engineers and other data scientists\n",
    "Help model unstructured data sets\n",
    "You will be creating and delivering Data Science/NLP projects regularly.\n",
    "You will be effectively collaborating with colleagues to solve business problems.\n",
    "Build NLP processing pipelines\n",
    "Work on conducting proper testing to remove bias\n",
    "Apply state of the art NLP solutions to solve real-world problems\n",
    "Your Skills And Experience\n",
    "\n",
    "To be a fit for this position, you need to have:\n",
    "Strong knowledge of working with Python, SQL, and Python libraries\n",
    "Proven industry experience working with NLP tools like BERT, NLTK, GenSim, or similar\n",
    "Knowledge of Public Clouds, ideally AWS is nice to have\n",
    "Have a background in computational linguistics, text mining, topic modeling, semantic analysis or text classification, or similar\n",
    "Fluency in English is a must\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "69b17544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The job description is about: \n",
      "\t\t\t Mechanical Engineer\n"
     ]
    }
   ],
   "source": [
    "#What is the job description about\n",
    "\n",
    "index=model.predict([job_description]).argmax()\n",
    "candidate_resume=resume_df[resume_df[\"Category\"]==index][\"Resume\"]\n",
    "\n",
    "print(\"The job description is about: \\n\\t\\t\\t\",dict[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "d230fd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7579564e-04, 2.4757410e-06, 1.1377738e-05, 2.2445786e-04,\n",
       "        1.1876457e-01, 2.2562090e-01, 2.2589774e-03, 6.6879060e-04,\n",
       "        1.0431462e-05, 2.2538290e-03, 1.5840247e-04, 1.6940762e-06,\n",
       "        2.3348026e-07, 4.5474612e-06, 2.3147702e-06, 5.9800595e-07,\n",
       "        4.3587261e-01, 5.2241944e-07, 8.3095962e-05, 1.6385786e-03,\n",
       "        5.6193457e-03, 2.4762666e-03, 1.4116225e-01, 4.5851846e-03,\n",
       "        5.8202773e-02]], dtype=float32)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the probabilites of a job to be part of each category of the dataset\n",
    "job_prop=model.predict([job_description])\n",
    "# Find the probabilites of the candidate resume to each category of the dataset\n",
    "candidate_prob=model.predict(candidate_resume)\n",
    "# candidate_prob\n",
    "job_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74596d2",
   "metadata": {},
   "source": [
    "# I need word embedding model to make a cosine similarity between a job description and the candidate resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "974305e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity=cosine_similarity(job_prop,candidate_prob)\n",
    "similarity_tupled=list(zip(similarity.tolist()[0],candidate_resume.index))\n",
    "similarity_tupled=sorted(similarity_tupled,key=lambda x:x[0],reverse=True)\n",
    "top_rated=similarity_tupled[:5]\n",
    "top_rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "02fe9044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top five rated resume: \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Mechanical Engineer</td>\n",
       "      <td>Education Details \\r\\nJanuary 2018 Bachelor's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Mechanical Engineer</td>\n",
       "      <td>Education Details \\r\\nJanuary 2018 Bachelor's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Mechanical Engineer</td>\n",
       "      <td>Education Details \\r\\nJanuary 2018 Bachelor's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Mechanical Engineer</td>\n",
       "      <td>Education Details \\r\\nJanuary 2018 Bachelor's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Mechanical Engineer</td>\n",
       "      <td>Education Details \\r\\nJanuary 2018 Bachelor's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Category                                             Resume\n",
       "187  Mechanical Engineer  Education Details \\r\\nJanuary 2018 Bachelor's ...\n",
       "202  Mechanical Engineer  Education Details \\r\\nJanuary 2018 Bachelor's ...\n",
       "212  Mechanical Engineer  Education Details \\r\\nJanuary 2018 Bachelor's ...\n",
       "217  Mechanical Engineer  Education Details \\r\\nJanuary 2018 Bachelor's ...\n",
       "192  Mechanical Engineer  Education Details \\r\\nJanuary 2018 Bachelor's ..."
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The top five rated resume: \\n\\n\")\n",
    "top_rated_indices=[index[1] for index in top_rated]\n",
    "resume_copied.iloc[top_rated_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30380681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f3bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40073b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9985d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92747ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905eb3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
